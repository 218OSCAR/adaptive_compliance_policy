{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d095e58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import matplotlib\n",
    "%matplotlib widget\n",
    "print(\"\"\"\n",
    "╔════════════════════════════════════════════════════════════════╗\n",
    "║   SAM2 Video Segmentation: Remove Arm & Gripper from Kinect   ║\n",
    "╚════════════════════════════════════════════════════════════════╝\n",
    "\n",
    "This notebook performs interactive arm removal from video:\n",
    "\n",
    "STEP 1: Interactive Point Annotation\n",
    "  - Click on arm/gripper in the video to add POS points\n",
    "  - Click on background to add NEG (negative) points\n",
    "  - Use these points as prompts for SAM2\n",
    "\n",
    "STEP 2: SAM2 Video Segmentation\n",
    "  - Uses your point prompts to segment arm/gripper across video\n",
    "  - Produces frame-by-frame segmentation masks\n",
    "\n",
    "STEP 3: Remove Arm using Background\n",
    "  - Replaces masked regions with first frame background\n",
    "  - Keeps scene with clean arm-free images\n",
    "\n",
    "STEP 4: Visualization\n",
    "  - Shows before/after comparison for selected frames\n",
    "\n",
    "Output:\n",
    "  - Final video saved to: VIDEO_PATH_FINAL\n",
    "  - Processed frames array: frames_no_arm\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ce5333",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def unwrap_obj(x):\n",
    "    if isinstance(x, np.ndarray) and x.dtype == object:\n",
    "        try:\n",
    "            return x.item()\n",
    "        except Exception:\n",
    "            return x.tolist()\n",
    "    return x\n",
    "\n",
    "def decode_kinect_frame(frame):\n",
    "    \"\"\"\n",
    "    兼容你数据格式：\n",
    "    frame 是 dict，包含 \"data\" -> jpeg bytes (np.ndarray uint8 1D / bytes)\n",
    "    \"\"\"\n",
    "    frame = unwrap_obj(frame)\n",
    "\n",
    "    if isinstance(frame, dict) and \"data\" in frame:\n",
    "        buf = frame[\"data\"]\n",
    "        if isinstance(buf, np.ndarray):\n",
    "            buf = buf.tobytes()\n",
    "        elif isinstance(buf, (bytes, bytearray)):\n",
    "            buf = bytes(buf)\n",
    "        else:\n",
    "            buf = bytes(np.array(buf, dtype=np.uint8))\n",
    "\n",
    "        arr = np.frombuffer(buf, dtype=np.uint8)\n",
    "        bgr = cv2.imdecode(arr, cv2.IMREAD_COLOR)\n",
    "        if bgr is None:\n",
    "            raise ValueError(\"cv2.imdecode failed for this frame.\")\n",
    "        rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
    "        return rgb\n",
    "\n",
    "    # 兜底：如果已经是图像数组\n",
    "    if isinstance(frame, np.ndarray) and frame.ndim == 3 and frame.shape[-1] == 3:\n",
    "        if frame.dtype != np.uint8:\n",
    "            frame = np.clip(frame, 0, 255).astype(np.uint8)\n",
    "        return frame\n",
    "\n",
    "    raise TypeError(f\"Unsupported kinect frame type: {type(frame)}\")\n",
    "\n",
    "def load_kinect_rgb_frames(npz_path, max_frames=None):\n",
    "    data = np.load(npz_path, allow_pickle=True)\n",
    "    if \"kinect_rgb\" not in data.files:\n",
    "        raise KeyError(f\"'kinect_rgb' not found. Available keys: {list(data.files)}\")\n",
    "\n",
    "    frames_raw = data[\"kinect_rgb\"]\n",
    "    T = len(frames_raw)\n",
    "    if max_frames is not None:\n",
    "        T = min(T, max_frames)\n",
    "\n",
    "    frames = []\n",
    "    for i in range(T):\n",
    "        frames.append(decode_kinect_frame(frames_raw[i]))\n",
    "    frames = np.stack(frames, axis=0)  # (T,H,W,3) uint8 RGB\n",
    "    return frames, data\n",
    "\n",
    "# ====== TODO 改成你的npz路径 ======\n",
    "\n",
    "# NPZ_PATH = \"/mnt/WDC10T/tailai_ws/dataset/one_clip_mounting/data_processed_ft/episode_20260227_152342_a7d90d_processed.npz\"\n",
    "root = os.environ.get(\"PIXI_PROJECT_ROOT\", \"\")\n",
    "npz_name = 'episode_20260206_162408_109a4a_processed'\n",
    "\n",
    "NPZ_PATH = os.path.join(root, \"data\", \"real\", npz_name + \".npz\")\n",
    "\n",
    "#  TODO: 改成你的SAM2 路径!!!!\n",
    "CKPT_PATH = Path(\"/sam2.1_hiera_large.pt\").expanduser()\n",
    "assert CKPT_PATH.exists(), f\"ckpt not found: {CKPT_PATH}\"\n",
    "\n",
    "frames, npz_data = load_kinect_rgb_frames(NPZ_PATH)\n",
    "print(\"frames:\", frames.shape, frames.dtype)\n",
    "\n",
    "# 第一帧无人&无gripper -> 静态背景参考\n",
    "bg_ref = frames[0].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aea10d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "print(\"Creating video using imageio...\")\n",
    "\n",
    "# 先安装 imageio-ffmpeg\n",
    "import subprocess\n",
    "try:\n",
    "    import imageio\n",
    "except ImportError:\n",
    "    print(\"Installing imageio-ffmpeg...\")\n",
    "    subprocess.check_call([\"pip\", \"-q\", \"install\", \"imageio\", \"imageio-ffmpeg\"])\n",
    "    import imageio\n",
    "\n",
    "h, w = frames.shape[1], frames.shape[2]\n",
    "num_frames = len(frames)\n",
    "\n",
    "print(f\"Video: {w}x{h} @ 15fps, {num_frames} frames\")\n",
    "print(f\"Frame dtype: {frames.dtype}\")\n",
    "\n",
    "tmp_dir = Path(tempfile.mkdtemp())\n",
    "VIDEO_PATH = tmp_dir / \"kinect_rgb.mp4\"\n",
    "\n",
    "print(f\"\\nWriting to {VIDEO_PATH}...\")\n",
    "\n",
    "# imageio 接收 RGB 帧，会自动处理编码\n",
    "with imageio.get_writer(str(VIDEO_PATH), fps=15, codec='libx264', pixelformat='yuv420p') as writer:\n",
    "    for i, frame in enumerate(frames):\n",
    "        writer.append_data(frame)\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f\"  {i+1}/{num_frames}\")\n",
    "\n",
    "file_size = os.path.getsize(VIDEO_PATH)\n",
    "print(f\"\\n✓ Video saved successfully!\")\n",
    "print(f\"✓ File size: {file_size / 1024 / 1024:.2f} MB\")\n",
    "print(f\"✓ All {num_frames} frames written\")\n",
    "print(f\"\\nVideo path: {VIDEO_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50998b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# 只保留帧选择 + 清点按钮\n",
    "idx_slider = widgets.IntSlider(\n",
    "    min=0, max=len(frames)-1, step=1,\n",
    "    value=min(10, len(frames)-1),\n",
    "    description=\"pick_frame\"\n",
    ")\n",
    "clear_btn = widgets.Button(description=\"Clear points\")\n",
    "display(widgets.HBox([idx_slider, clear_btn]))\n",
    "\n",
    "# 只存 positive 点\n",
    "click_state = {\"pos\": []}\n",
    "\n",
    "fig = go.FigureWidget()\n",
    "fig.update_layout(width=900, height=550, margin=dict(l=10, r=10, t=10, b=10))\n",
    "\n",
    "img_trace = go.Image(z=frames[idx_slider.value])\n",
    "pos_scatter = go.Scatter(\n",
    "    x=[], y=[],\n",
    "    mode=\"markers\",\n",
    "    marker=dict(symbol=\"cross\", size=12),\n",
    "    name=\"POS\"\n",
    ")\n",
    "fig.add_trace(img_trace)\n",
    "fig.add_trace(pos_scatter)\n",
    "\n",
    "def refresh_points():\n",
    "    with fig.batch_update():\n",
    "        fig.data[1].x = [p[0] for p in click_state[\"pos\"]]\n",
    "        fig.data[1].y = [p[1] for p in click_state[\"pos\"]]\n",
    "\n",
    "def on_click(trace, points, selector):\n",
    "    if len(points.xs) == 0:\n",
    "        return\n",
    "    x, y = float(points.xs[0]), float(points.ys[0])\n",
    "    click_state[\"pos\"].append((x, y))\n",
    "    refresh_points()\n",
    "\n",
    "fig.data[0].on_click(on_click)\n",
    "\n",
    "def on_change_idx(change):\n",
    "    # 切换帧时清点（更直观；如果你想跨帧保留点，把 clear 那两行删掉）\n",
    "    global PROMPT_FRAME_IDX\n",
    "    PROMPT_FRAME_IDX = int(change[\"new\"])\n",
    "    click_state[\"pos\"].clear()\n",
    "    with fig.batch_update():\n",
    "        fig.data[0].z = frames[change[\"new\"]]\n",
    "    refresh_points()\n",
    "\n",
    "idx_slider.observe(on_change_idx, names=\"value\")\n",
    "\n",
    "def on_clear(_):\n",
    "    click_state[\"pos\"].clear()\n",
    "    refresh_points()\n",
    "\n",
    "use_btn = widgets.Button(description=\"Use this frame\")\n",
    "display(use_btn)\n",
    "\n",
    "clear_btn.on_click(on_clear)\n",
    "\n",
    "def on_use(_):\n",
    "    global PROMPT_FRAME_IDX, POS_POINTS\n",
    "    PROMPT_FRAME_IDX = int(idx_slider.value)\n",
    "    POS_POINTS = list(click_state[\"pos\"])\n",
    "    print(\"Locked:\", PROMPT_FRAME_IDX, \"points:\", len(POS_POINTS))\n",
    "\n",
    "use_btn.on_click(on_use)\n",
    "\n",
    "\n",
    "\n",
    "display(fig)\n",
    "\n",
    "print(\"提示：点击图片取 POS 点；点会被存到 click_state['pos']。点完直接运行下一格。\")\n",
    "\n",
    "POS_POINTS = click_state[\"pos\"]  # 你后面用这个就行\n",
    "PICK_FRAME_SLIDER = idx_slider\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b484dc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import imageio.v2 as imageio\n",
    "\n",
    "\n",
    "root = os.environ.get(\"PIXI_PROJECT_ROOT\", \"\")\n",
    "\n",
    "BASE_DIR = os.path.join(root, \"data_processed_script/segment/predict\")\n",
    "VIDEO_DIR = os.path.join(BASE_DIR, \"frames\")\n",
    "os.makedirs(VIDEO_DIR, exist_ok=True)\n",
    "\n",
    "# 写成 00000.jpg, 00001.jpg ...\n",
    "for i, img in enumerate(frames):\n",
    "    imageio.imwrite(os.path.join(VIDEO_DIR, f\"{i:05d}.jpg\"), img)\n",
    "\n",
    "print(\"Saved frames to:\", VIDEO_DIR, \" num_frames =\", len(frames))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0fdf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "from sam2.build_sam import build_sam2_video_predictor\n",
    "\n",
    "\n",
    "\n",
    "MODEL_CFG = \"configs/sam2.1/sam2.1_hiera_l.yaml\"  # ✅ 你 repo 里官方就是这么写\n",
    "predictor = build_sam2_video_predictor(MODEL_CFG, str(CKPT_PATH), device=\"cuda\")\n",
    "\n",
    "\n",
    "print(\"CUDA:\", torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd799d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "with torch.inference_mode(), torch.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "    state = predictor.init_state(VIDEO_DIR)\n",
    "\n",
    "print(\"State initialized.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb20f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"PROMPT_FRAME_IDX =\", PROMPT_FRAME_IDX)\n",
    "print(\"num POS points =\", len(POS_POINTS))\n",
    "\n",
    "print(\"POS_POINTS sample =\", POS_POINTS[:6])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa52d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "ann_frame_idx = int(PROMPT_FRAME_IDX)\n",
    "\n",
    "points = np.array(POS_POINTS, dtype=np.float32)                # (K,2)\n",
    "labels = np.ones((len(POS_POINTS),), dtype=np.int32)           # (K,) 全是正点\n",
    "\n",
    "HAND_OBJ_ID = 1\n",
    "\n",
    "with torch.inference_mode(), torch.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "    frame_idx, obj_ids, mask_logits = predictor.add_new_points_or_box(\n",
    "        state,\n",
    "        frame_idx=ann_frame_idx,\n",
    "        obj_id=HAND_OBJ_ID,\n",
    "        points=points,\n",
    "        labels=labels,\n",
    "    )\n",
    "\n",
    "print(\"Added points on frame:\", frame_idx, \" obj_ids:\", [int(x) for x in obj_ids])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3a57c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "obj_ids_list = [int(x) for x in obj_ids]\n",
    "j = obj_ids_list.index(HAND_OBJ_ID)\n",
    "\n",
    "ml = mask_logits[j].detach().cpu().numpy()   # 可能是 (1,H,W) 或 (H,W)\n",
    "\n",
    "# 关键：变成 (H,W)\n",
    "if ml.ndim == 3 and ml.shape[0] == 1:\n",
    "    ml = ml[0]\n",
    "elif ml.ndim == 3:\n",
    "    # 极端情况（比如 (T,H,W)），取第一张先可视化\n",
    "    ml = ml[0]\n",
    "\n",
    "m = (ml > 0)  # bool (H,W)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.imshow(frames[ann_frame_idx])\n",
    "plt.imshow(m.astype(np.float32), alpha=0.5)  # 转 float 更保险\n",
    "plt.title(f\"Hand mask preview | frame {ann_frame_idx} | pos_points={len(POS_POINTS)} | mask shape={m.shape}\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a3a4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob\n",
    "\n",
    "VIDEO_DIR = \"/home/tailai.cheng/multimodal_flowmatching/scripts/runs/segment/predict/frames\"\n",
    "\n",
    "jpgs = sorted(glob.glob(os.path.join(VIDEO_DIR, \"*.jpg\")))\n",
    "print(\"num jpgs =\", len(jpgs))\n",
    "print(\"first =\", os.path.basename(jpgs[0]) if jpgs else None)\n",
    "print(\"last  =\", os.path.basename(jpgs[-1]) if jpgs else None)\n",
    "\n",
    "# 看看是否断号（比如缺 00237.jpg）\n",
    "if jpgs:\n",
    "    idxs = [int(os.path.splitext(os.path.basename(p))[0]) for p in jpgs]\n",
    "    print(\"min idx =\", min(idxs), \"max idx =\", max(idxs))\n",
    "    missing = sorted(set(range(min(idxs), max(idxs)+1)) - set(idxs))\n",
    "    print(\"missing count =\", len(missing))\n",
    "    print(\"first missing few =\", missing[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac64d806",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def logits_to_mask_hw(mask_logit_tensor):\n",
    "    \"\"\"\n",
    "    Convert SAM2 mask logits tensor to (H,W) boolean mask.\n",
    "    Handles shapes like:\n",
    "      (H, W)\n",
    "      (1, H, W)\n",
    "      (T, H, W) -> take first\n",
    "    \"\"\"\n",
    "    ml = mask_logit_tensor.detach().cpu().numpy()\n",
    "\n",
    "    if ml.ndim == 3 and ml.shape[0] == 1:\n",
    "        ml = ml[0]\n",
    "    elif ml.ndim == 3:\n",
    "        ml = ml[0]\n",
    "    elif ml.ndim != 2:\n",
    "        raise ValueError(f\"Unexpected mask logit shape: {ml.shape}\")\n",
    "\n",
    "    return (ml > 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b955255",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "masks_all = {}\n",
    "\n",
    "# 1) forward: 从 ann_frame_idx 往后\n",
    "with torch.inference_mode(), torch.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "    for f_idx, obj_ids, mask_logits in predictor.propagate_in_video(state):\n",
    "        obj_ids_list = [int(x) for x in obj_ids]\n",
    "        if HAND_OBJ_ID not in obj_ids_list:\n",
    "            continue\n",
    "        j = obj_ids_list.index(HAND_OBJ_ID)\n",
    "        m = logits_to_mask_hw(mask_logits[j])\n",
    "        masks_all[int(f_idx)] = m\n",
    "\n",
    "print(\"after forward:\", len(masks_all))\n",
    "\n",
    "# 2) backward: 从 ann_frame_idx 往前\n",
    "with torch.inference_mode(), torch.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "    for f_idx, obj_ids, mask_logits in predictor.propagate_in_video(state, reverse=True):\n",
    "        obj_ids_list = [int(x) for x in obj_ids]\n",
    "        if HAND_OBJ_ID not in obj_ids_list:\n",
    "            continue\n",
    "        j = obj_ids_list.index(HAND_OBJ_ID)\n",
    "        m = logits_to_mask_hw(mask_logits[j])\n",
    "        masks_all[int(f_idx)] = m\n",
    "\n",
    "print(\"after backward:\", len(masks_all), \"/\", len(frames))\n",
    "print(\"min frame in masks:\", min(masks_all.keys()), \"max:\", max(masks_all.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1254e67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "OUT_DIR = \"/home/tailai.cheng/multimodal_flowmatching/scripts/runs/segment/predict\"\n",
    "OUT_VIDEO = os.path.join(OUT_DIR, \"hand_seg_overlay.mp4\")\n",
    "\n",
    "h, w, _ = frames[0].shape\n",
    "fps = 15  # 你也可以改成原视频 fps\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "writer = cv2.VideoWriter(OUT_VIDEO, fourcc, fps, (w, h))\n",
    "\n",
    "for i in range(len(frames)):\n",
    "    img = frames[i].copy()\n",
    "    m = masks_all.get(i, None)\n",
    "\n",
    "    if m is not None:\n",
    "        # 红色 overlay（你可以改颜色）\n",
    "        overlay = img.copy()\n",
    "        overlay[m] = (255, 0, 0)\n",
    "        img = cv2.addWeighted(overlay, 0.5, img, 0.5, 0)\n",
    "\n",
    "    writer.write(img[:, :, ::-1])  # RGB -> BGR (cv2)\n",
    "\n",
    "writer.release()\n",
    "print(\"Saved video to:\", OUT_VIDEO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1dc613e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "OUT_DIR = \"/home/tailai.cheng/multimodal_flowmatching/scripts/runs/segment/predict\"\n",
    "OUT_VIDEO_NOHAND = os.path.join(OUT_DIR, \"video_no_hand_bgfill.mp4\")\n",
    "\n",
    "h, w, _ = frames[0].shape\n",
    "fps = 15  # 按你的设置\n",
    "\n",
    "# 背景参考帧：第一帧（你确认手未进入）\n",
    "bg = frames[0].copy()  # RGB (H,W,3)\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "writer = cv2.VideoWriter(OUT_VIDEO_NOHAND, fourcc, fps, (w, h))\n",
    "\n",
    "# 可选：给 mask 做一点点“膨胀”以覆盖边缘（手边缘常有漏/毛刺）\n",
    "KERNEL = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))  # 你可改成 (3,3)/(7,7)\n",
    "\n",
    "for i in range(len(frames)):\n",
    "    img = frames[i].copy()  # RGB\n",
    "    m = masks_all.get(i, None)\n",
    "\n",
    "    if m is not None:\n",
    "        m_u8 = (m.astype(np.uint8) * 255)\n",
    "        m_u8 = cv2.dilate(m_u8, KERNEL, iterations=1)  # 可注释掉这一行看差别\n",
    "        m_bool = m_u8 > 0  # (H,W) bool\n",
    "\n",
    "        # 用背景像素替换手区域\n",
    "        img[m_bool] = bg[m_bool]\n",
    "\n",
    "    writer.write(img[:, :, ::-1])  # RGB -> BGR\n",
    "writer.release()\n",
    "\n",
    "print(\"Saved no-hand video to:\", OUT_VIDEO_NOHAND)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
